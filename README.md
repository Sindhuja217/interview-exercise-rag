# ğŸ§  Support Knowledge Assistant (RAG + MCP)

A **production-grade Retrieval-Augmented Generation (RAG) system** for resolving customer support tickets using **structured knowledge**, **hybrid retrieval**, and **deterministic action inference**.

This system is designed for **real-world support operations**, providing grounded answers, explicit references, and predictable escalation behaviorâ€”without relying on fragile, end-to-end LLM reasoning.

---

## ğŸš€ Overview

The Support Knowledge Assistant ingests internal markdown documentation (FAQs, policies, runbooks) and exposes a FastAPI service that resolves customer support tickets end-to-end.

For every ticket, the system returns:
- âœ… A **clear, accurate answer** grounded in retrieved documentation  
- ğŸ“ **Top-K document references** for auditability  
- âš ï¸ A **deterministic action recommendation** (follow-up, escalation, or customer action)

The knowledge base is **fully dynamic**â€”documents can be updated or replaced without retraining any model.

---

## âœ¨ Key Capabilities

- Hybrid retrieval (**dense + BM25 sparse**) using Qdrant  
- Cross-encoder reranking for high-precision relevance  
- Query rewriting for retrieval robustness  
- Deterministic action inference via semantic similarity  
- MCP-style structured responses with strict schema validation  
- Provider-agnostic LLM interface (OpenAI)  
- Docker-ready for production deployment  
- Streamlit version of the app is also available

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    %% Knowledge Ingestion
    A[Markdown Knowledge Base]
        --> B[Hierarchial Chunking]

    B --> C[Structured Chunks]

    C --> D[Metadata Enrichment]

    D --> E[Dense Embeddings]
    D --> F[Sparse Embeddings]

    E --> G[Hybrid Index]
    F --> G

    G --> H[Qdrant Vector Store]

    %% Query and Retrieval
    I[Customer Ticket]
        --> J[Query Rewriting]

    J --> K[Search Queries]

    K --> L[Hybrid Retrieval]

    L --> M[Candidate Documents]

    M --> N[Cross Encoder Reranking]

    N --> O[Ranked Context]

    O --> P[Final Context Selection]

    %% Answer and Decision
    P --> Q[LLM Answer Generation]

    Q --> R[Answer JSON]

    R --> S[Action Inference]

    S --> T[Schema Validation]

    T --> U[Final Support Response]

```

---

## ğŸ§© Core Design Principles

### 1. Retrieval Before Generation
The LLM never answers blindly. All responses are generated **only after retrieving and ranking relevant documentation**.

### 2. Deterministic Actions (No LLM Guessing)
Escalation and follow-up decisions are **not generated by the LLM**.  
They are inferred using semantic similarity against curated action prototypes.

### 3. Strict Output Contracts
All outputs are validated using **Pydantic schemas**.  
Malformed or hallucinated responses fail fast.

### 4. Purposeful Model Selection (Why gpt-4o-mini)

The system uses gpt-4o-mini by designâ€”not as a default.
We chose this model because:
- Strong instruction-following for strict JSON-only generation
- Low latency and cost, suitable for high-volume support workloads
- Sufficient reasoning capability when paired with high-quality retrieved context
- Reliability over creativity, which is critical for customer support systems
- Because retrieval, validation, and action inference are handled outside the model, the system does not require a large or creative LLMâ€”only a consistent, controllable generator.
---

## âš™ï¸ Technology Stack

| Layer | Technology |
|-----|-----------|
| API | FastAPI |
| LLM | OpenAI (default) |
| Retrieval | Qdrant (Hybrid Dense + Sparse) |
| Dense Embeddings | all-MiniLM-L6-v2 |
| Sparse Search | BM25 |
| Reranking | Cross-Encoder (ms-marco-MiniLM-L-6-v2O) |
| Validation | Pydantic (MCP-style schemas) |
| Deployment | Docker |

---

## ğŸ“ Repository Structure

```text
interview-exercise-rag/
â”œâ”€â”€ artifacts/
â”‚   â””â”€â”€ langchain_chunks.txt        # Auditable output of markdown chunking
â”‚
â”œâ”€â”€ data/                           # Knowledge base (source of truth)
â”‚   â”œâ”€â”€ faqs/                       # Frequently asked questions
â”‚   â”œâ”€â”€ policies/                   # Policy documents
â”‚   â””â”€â”€ runbooks/                   # Operational runbooks
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ action_classifier.py    # Deterministic action inference (semantic)
â”‚       â”œâ”€â”€ chunking.py             # Header-aware markdown chunking
â”‚       â”œâ”€â”€ embedding.py            # Dense + sparse embedding & indexing
â”‚       â”œâ”€â”€ generation.py           # MCP-compliant answer generation
â”‚       â”œâ”€â”€ llm_client.py           # Unified OpenAI / Ollama client
â”‚       â”œâ”€â”€ prompts.py              # Prompt templates (generation, rewrite)
â”‚       â”œâ”€â”€ query_rewriter.py       # Retrieval-optimized query rewriting
â”‚       â”œâ”€â”€ rag_pipeline.py         # End-to-end RAG orchestration
â”‚       â”œâ”€â”€ references.py           # Stable, human-readable references
â”‚       â”œâ”€â”€ retriever.py            # Hybrid retrieval + reranking
â”‚       â”œâ”€â”€ schemas.py              # Strict Pydantic MCP schemas
â”‚       â””â”€â”€ main.py                 # FastAPI application entrypoint
â”‚
â”œâ”€â”€ tests/                          # Unit and integration tests
â”œâ”€â”€ venv/                           # Local virtual environment (not committed)
â”‚
â”œâ”€â”€ .env                            # Environment variables (local)
â”œâ”€â”€ app.py                          # Streamlit app entry (if used externally)
â”œâ”€â”€ docker-compose.yml              # Local multi-service orchestration
â”œâ”€â”€ Dockerfile                      # Production container build
â”œâ”€â”€ pytest.ini                      # Pytest configuration
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # Project documentation
```

---

## ğŸ”„ End-to-End Ticket Flow

1. Ticket rewriting into retrieval-optimized queries  
2. Hybrid dense + sparse retrieval from Qdrant  
3. Cross-encoder reranking with relevance scoring  
4. Context-grounded LLM answer generation  
5. Deterministic action inference  
6. MCP schema validation  

---

## ğŸ”§ Installation & Setup

### Clone the Repository
```bash
git clone https://github.com/Sindhuja217/interview-exercise-rag.git
cd interview-exercise-rag
```

### Run with Docker
```bash
# Start all services 
docker compose up --build

# View logs
docker compose logs -f

# Stop all services
docker compose down
```
Once the server is up and running, open the following URL in your browser to access the interactive session:

ğŸ‘‰ http://0.0.0.0:8000/docs
- Submit support tickets
- Inspect request/response schemas
- Test the RAG pipeline end-to-end


### Run with Streamlit
```bash
streamlit run app.py
```

---

## ğŸŒ API Endpoints

### Health Check
```bash
GET /health
```

### Resolve Support Ticket
```bash
POST /resolve-ticket
```

Request:
```json
{
  "ticket_text": "My domain was suspended yesterday. How do I reactivate it?"
}
```

Response:
```json
{
  "answer": "Your domain may have been suspended due to an abuse report...",
  "references": [
    "runbooks: Domain Suspension Â§ Abuse Review | file=runbooks/domain_suspension.md"
  ],
  "action_required": "escalate_to_abuse_team"
}
```
Sample questions are provided in `questions.md` file.

---

#### Copy the example environment file and change the file name to `.env`

```bash
cp .env.example .env
```

## ğŸ§ª Running Tests

### 1. Activate Python environment
```bash
python3 -m venv .venv
source venv/bin/activate 
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Run test suite
```bash
pytest
```
### 4. Copy the example environment file and fill the credentials to run the code

```bash
cp .env.example .env
```
---

## ğŸ”’ Reliability Guarantees

- No hallucinated escalations  
- No undocumented actions  
- Max 3 references enforced  
- Schema-validated responses only  

---

## ğŸ‘¤ Author

**Sindhuja Chaduvula**  
*Machine Learning Specialist, Vector Institute*
ğŸ“§ sindhu.chaduvula.21@gmail.com
ğŸ”— [LinkedIn](https://www.linkedin.com/in/sindhuja-chaduvula/) | [Portfolio](https://sindhuja217.github.io/sindhujachaduvula/)
